{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 6\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori=pd.read_csv('website_data_20190225.csv')\n",
    "ori.drop(['STATE','DISTRICT','WLCODE','SITE_TYPE','TEH_NAME'],axis=1,inplace=True)\n",
    "ori.replace(to_replace=\"'0\",value=0,inplace=True)\n",
    "# ori.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.DataFrame().reindex_like(ori)\n",
    "dataset1=pd.DataFrame().reindex_like(ori)\n",
    "dataset.dropna(inplace=True)\n",
    "dataset1.dropna(inplace=True)\n",
    "\n",
    "# j=0\n",
    "# for i in range(0,ori.shape[0]):\n",
    "#     if ori['STATE'][i]=='RJ':\n",
    "#         dataset.loc[j] = ori.iloc[i]\n",
    "#         j+=1\n",
    "# dataset.drop(['STATE'],axis=1,inplace=True)\n",
    "\n",
    "# j=0\n",
    "# for i in range(0,ori.shape[0]):\n",
    "#     if ori['DISTRICT'][i]=='Ajmer':\n",
    "#         dataset.loc[j] = ori.iloc[i]\n",
    "#         j+=1\n",
    "# dataset.drop(['DISTRICT'],axis=1,inplace=True)\n",
    "\n",
    "j=0\n",
    "for i in range(0,ori.shape[0]):\n",
    "    if ori['BLOCK_NAME'][i]=='Arain':\n",
    "        dataset1.loc[j] = ori.iloc[i]\n",
    "        j+=1\n",
    "dataset1.drop(['BLOCK_NAME'],axis=1,inplace=True)\n",
    "dataset.drop(['BLOCK_NAME'],axis=1,inplace=True)\n",
    "\n",
    "j=0\n",
    "for i in range(0,dataset1.shape[0]):\n",
    "    if dataset1['SITE_NAME'][i]=='Sanpla':\n",
    "        dataset.loc[j] = dataset1.iloc[i]\n",
    "        j+=1\n",
    "lat=dataset[\"LAT\"][0]\n",
    "lon=dataset[\"LON\"][0]\n",
    "dataset.drop(['SITE_NAME','LAT','LON'],axis=1,inplace=True)\n",
    " \n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,dataset.shape[0]):\n",
    "    dataset['MONSOON'][i]=float(dataset['MONSOON'][i])\n",
    "    dataset['POMRB'][i]=float(dataset['POMRB'][i])\n",
    "    dataset['POMKH'][i]=float(dataset['POMKH'][i])\n",
    "    dataset['PREMON'][i]=float(dataset['PREMON'][i])\n",
    "    dataset['YEAR_OBS'][i]=int(dataset['YEAR_OBS'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "first=list(dataset['MONSOON'])\n",
    "second=list(dataset['POMRB'])\n",
    "third=list(dataset['POMKH'])\n",
    "fourth=list(dataset['PREMON'])\n",
    "dataset['MONSOON']=pd.core.frame.DataFrame(x+y+z+w for x, y,z,w in zip(first, second, third, fourth))\n",
    "dataset.drop(['POMRB','POMKH','PREMON'],axis=1,inplace=True)\n",
    "dataset = dataset.iloc[::-1]\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['YEAR_OBS']=(dataset['YEAR_OBS']).apply(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['YEAR_OBS']=pd.to_datetime(dataset['YEAR_OBS'],yearfirst=True,format='%Y',infer_datetime_format=True)\n",
    "indexedDataset=dataset.set_index(['YEAR_OBS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# indexedDataset.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.xlabel('Years')\n",
    "# plt.ylabel('Water-Level')\n",
    "# plt.plot(indexedDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A stationary time series is one whose statistical properties such as mean, variance, autocorrelation, etc. are all constant over time. Most statistical forecasting methods are based on the assumption that the time series can be rendered approximately stationary (i.e., \"stationarized\") through the use of mathematical transformations. A stationarized series is relatively easy to predict: you simply predict that its statistical properties will be the same in the future as they have been in the past!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can check stationarity using the following:\n",
    "\n",
    "- - Plotting Rolling Statistics: We can plot the moving average or moving variance and see if it varies with time. This is more of a visual technique.\n",
    "- - Dickey-Fuller Test: This is one of the statistical tests for checking stationarity. Here the null hypothesis is that the TimeSeries is non-stationary. The test results comprise of a Test Statistic and some Critical Values for difference confidence levels. If the ‘Test Statistic’ is less than the ‘Critical Value’, we can reject the null hypothesis and say that the series is stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "def test_stationary(timeseries):\n",
    "    \n",
    "    #Determing rolling statistics\n",
    "    moving_average=timeseries.rolling(window=12).mean()\n",
    "    standard_deviation=timeseries.rolling(window=12).std()\n",
    "    \n",
    "    #Plot rolling statistics:\n",
    "#     plt.plot(timeseries,color='blue',label=\"Original\")\n",
    "#     plt.plot(moving_average,color='red',label='Mean')\n",
    "#     plt.plot(standard_deviation,color='black',label='Standard Deviation')\n",
    "#     plt.legend(loc='best')  #best for axes\n",
    "#     plt.title('Rolling Mean & Deviation')\n",
    "#     plt.show()\n",
    "    plt.show(block=False)\n",
    "    \n",
    "    #Perform Dickey-Fuller test:\n",
    "#     print('Results Of Dickey-Fuller Test')\n",
    "    tstest=adfuller(timeseries['MONSOON'],autolag='AIC')\n",
    "    tsoutput=pd.Series(tstest[0:4],index=['Test Statistcs','P-value','#Lags used',\"#Obs. used\"])\n",
    "    #Test Statistics should be less than the Critical Value for Stationarity\n",
    "    #lesser the p-value, greater the stationarity\n",
    "    # print(list(dftest))\n",
    "    for key,value in tstest[4].items():\n",
    "        tsoutput['Critical Value (%s)'%key]=value\n",
    "#     print((tsoutput))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stationary(indexedDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are 2 major reasons behind non-stationaruty of a TS:\n",
    "- - Trend – varying mean over time. For eg, in this case we saw that on average, the number of passengers was growing over time.\n",
    "- - Seasonality – variations at specific time-frames. eg people might have a tendency to buy cars in a particular month because of pay increment or festivals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexed Dataset Logscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexedDataset_logscale=np.log(indexedDataset)\n",
    "test_stationary(indexedDataset_logscale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Log Minus Moving Average (dl_ma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolmeanlog=indexedDataset_logscale.rolling(window=12).mean()\n",
    "dl_ma=indexedDataset_logscale-rolmeanlog\n",
    "# dl_ma.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_ma.dropna(inplace=True)\n",
    "# dl_ma.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stationary(dl_ma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponential Decay Weighted Average (edwa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "edwa=indexedDataset_logscale.ewm(halflife=12,min_periods=0,adjust=True).mean()\n",
    "# plt.plot(indexedDataset_logscale)\n",
    "# plt.plot(edwa,color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Logscale Minus Moving Exponential Decay Average (dlmeda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlmeda=indexedDataset_logscale-edwa\n",
    "test_stationary(dlmeda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminating Trend and Seasonality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Differencing – taking the differece with a particular time lag\n",
    "- Decomposition – modeling both trend and seasonality and removing them from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Log Div Shifting (dlds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before Shifting\n",
    "# indexedDataset_logscale.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After Shifting\n",
    "# indexedDataset_logscale.shift().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlds=indexedDataset_logscale-indexedDataset_logscale.shift()\n",
    "dlds.dropna(inplace=True)\n",
    "test_stationary(dlds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "decompostion= seasonal_decompose(indexedDataset_logscale,freq=10)\n",
    "\n",
    "trend=decompostion.trend\n",
    "seasonal=decompostion.seasonal\n",
    "residual=decompostion.resid\n",
    "\n",
    "# plt.subplot(411)\n",
    "# plt.plot(indexedDataset_logscale,label='Original')\n",
    "# plt.legend(loc='best')\n",
    "\n",
    "# plt.subplot(412)\n",
    "# plt.plot(trend,label='Trend')\n",
    "# plt.legend(loc='best')\n",
    "\n",
    "# plt.subplot(413)\n",
    "# plt.plot(seasonal,label='Seasonal')\n",
    "# plt.legend(loc='best')\n",
    "\n",
    "# plt.subplot(414)\n",
    "# plt.plot(residual,label='Residual')\n",
    "# plt.legend(loc='best')\n",
    "\n",
    "# plt.tight_layout() #To Show Multiple Grpahs In One Output, Use plt.subplot(abc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here trend, seasonality are separated out from data and we can model the residuals. Lets check stationarity of residuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposedlogdata=residual\n",
    "decomposedlogdata.dropna(inplace=True)\n",
    "# test_stationary(decomposedlogdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting a Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ARIMA stands for Auto-Regressive Integrated Moving Averages. The ARIMA forecasting for a stationary time series is nothing but a linear (like a linear regression) equation. The predictors depend on the parameters (p,d,q) of the ARIMA model:\n",
    "\n",
    "- - Number of AR (Auto-Regressive) terms (p): AR terms are just lags of dependent variable. For instance if p is 5, the predictors for x(t) will be x(t-1)….x(t-5).\n",
    "- - Number of MA (Moving Average) terms (q): MA terms are lagged forecast errors in prediction equation. For instance if q is 5, the predictors for x(t) will be e(t-1)….e(t-5) where e(i) is the difference between the moving average at ith instant and actual value.\n",
    "- - Number of Differences (d): These are the number of nonseasonal differences, i.e. in this case we took the first order difference. So either we can pass that variable and put d=0 or pass the original variable and put d=1. Both will generate same results.\n",
    "\n",
    "\n",
    "- An importance concern here is how to determine the value of ‘p’ and ‘q’. We use two plots to determine these numbers.\n",
    "\n",
    "- - Autocorrelation Function (ACF): It is a measure of the correlation between the the TS with a lagged version of itself-. For instance at lag 5, ACF would compare series at time instant ‘t1’…’t2’ with series at instant ‘t1-5’…’t2-5’ (t1-5 and t2 being end points).\n",
    "- - Partial Autocorrelation Function (PACF): This measures the correlation between the TS with a lagged version of itself but after eliminating the variations already explained by the intervening comparisons. Eg at lag 5, it will check the correlation but remove the effects already explained by lags 1 to 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACF & PACF Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acf,pacf\n",
    "lag_acf=acf(dlds,nlags=20)\n",
    "lag_pacf=pacf(dlds,nlags=20,method='ols')\n",
    "\n",
    "# plt.subplot(121)\n",
    "# plt.plot(lag_acf)\n",
    "# plt.axhline(y=0, linestyle='--',color='gray')\n",
    "# plt.axhline(y=1.96/np.sqrt(len(dlds)),linestyle='--',color='gray')\n",
    "# plt.axhline(y=-1.96/np.sqrt(len(dlds)),linestyle='--',color='gray')\n",
    "# plt.title('AutoCorrelation Function')\n",
    "\n",
    "# plt.subplot(122)\n",
    "# plt.plot(lag_pacf)\n",
    "# plt.axhline(y=0, linestyle='--',color='gray')\n",
    "# plt.axhline(y=1.96/np.sqrt(len(dlds)),linestyle='--',color='gray')\n",
    "# plt.axhline(y=-1.96/np.sqrt(len(dlds)),linestyle='--',color='gray')\n",
    "# plt.title('PartialAutoCorrelation Function')\n",
    "\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this plot, the two dotted lines on either sides of 0 are the confidence interevals. These can be used to determine the ‘p’ and ‘q’ values as:\n",
    "\n",
    "- - p – The lag value where the PACF chart crosses the upper confidence interval for the first time. If we notice closely, in this case p=2.\n",
    "- - q – The lag value where the ACF chart crosses the upper confidence interval for the first time. If we notice closely, in this case q=2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "\n",
    "model=ARIMA(indexedDataset_logscale,order=(5,1,0))\n",
    "results_AR=model.fit(disp=-1)\n",
    "# plt.plot(dlds)\n",
    "# plt.plot(results_AR.fittedvalues,color='red')\n",
    "# plt.title('RSS: %.4f'%sum((results_AR.fittedvalues-dlds['MONSOON'])**2))\n",
    "# print('Plotting AR Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(indexedDataset_logscale, order=(0, 1, 2))  #0,1,2\n",
    "results_MA = model.fit(disp=-1)  \n",
    "# plt.plot(dlds)\n",
    "# plt.plot(results_MA.fittedvalues, color='red')\n",
    "# plt.title('RSS: %.4f'%sum((results_MA.fittedvalues-dlds['MONSOON'])**2))\n",
    "# print('Plotting MA Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(indexedDataset_logscale, order=(5, 1, 1))  \n",
    "results_ARIMA = model.fit(disp=-1)  \n",
    "# plt.plot(dlds)\n",
    "# plt.plot(results_ARIMA.fittedvalues, color='red')\n",
    "# plt.title('RSS: %.4f'%sum((results_ARIMA.fittedvalues-dlds['MONSOON'])**2))\n",
    "# print('Plotting Combined Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taking it back to original scale from residual scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the predicted results as a separate series\n",
    "predictions_ARIMA_diff = pd.Series(results_ARIMA.fittedvalues, copy=True)\n",
    "# predictions_ARIMA_diff.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notice that these start from ‘1949-02-01’ and not the first month. Why? This is because we took a lag by 1 and first element doesn’t have anything before it to subtract from. The way to convert the differencing to log scale is to add these differences consecutively to the base number. An easy way to do it is to first determine the cumulative sum at index and then add it to the base number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YEAR_OBS\n",
       "1995-01-01   -0.062128\n",
       "1996-01-01   -0.148681\n",
       "1997-01-01   -0.080785\n",
       "1998-01-01   -0.293591\n",
       "1999-01-01   -0.446932\n",
       "dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert to cummuative sum\n",
    "predictions_ARIMA_diff_cumsum = predictions_ARIMA_diff.cumsum()\n",
    "# predictions_ARIMA_diff_cumsum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YEAR_OBS\n",
       "1994-01-01    3.411478\n",
       "1995-01-01    3.349350\n",
       "1996-01-01    3.262797\n",
       "1997-01-01    3.330693\n",
       "1998-01-01    3.117886\n",
       "dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_ARIMA_log = pd.Series(indexedDataset_logscale['MONSOON'].ix[0], index=indexedDataset_logscale.index)\n",
    "predictions_ARIMA_log = predictions_ARIMA_log.add(predictions_ARIMA_diff_cumsum,fill_value=0)\n",
    "# predictions_ARIMA_log.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here the first element is base number itself and from there on the values cumulatively added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Last step is to take the exponent and compare with the original series.\n",
    "predictions_ARIMA = np.exp(predictions_ARIMA_log)\n",
    "# plt.plot(indexedDataset)\n",
    "# plt.plot(predictions_ARIMA)\n",
    "# plt.title('RMSE: %.4f'% np.sqrt(sum((predictions_ARIMA-indexedDataset['MONSOON'])**2)/len(indexedDataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finally we have a forecast at the original scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_ARIMA.plot_predict(1,26)\n",
    "\n",
    "#start = !st month\n",
    "#end = 10yrs forcasting = 144+12*10 = 264th month\n",
    "\n",
    "#Two models corresponds to AR & MA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([1.90599229, 1.69799919, 1.94723178, 1.63484492, 1.69498571]), array([0.4217554 , 0.43999561, 0.46503931, 0.4692189 , 0.47239288]), array([[1.07936689, 2.73261768],\n",
      "       [0.83562365, 2.56037474],\n",
      "       [1.03577148, 2.85869207],\n",
      "       [0.71519277, 2.55449707],\n",
      "       [0.76911268, 2.62085874]]))\n"
     ]
    }
   ],
   "source": [
    "x=results_ARIMA.forecast(steps=5)\n",
    "# print(x)\n",
    "#values in residual equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9059922882826494\t 0.42175540014923524\t [1.07936689 2.73261768]\n",
      "1.6979991933521283\t 0.4399956081180564\t [0.83562365 2.56037474]\n",
      "1.9472317783860675\t 0.4650393083562316\t [1.03577148 2.85869207]\n",
      "1.634844920955585\t 0.46921890104261377\t [0.71519277 2.55449707]\n",
      "1.6949857059390567\t 0.47239287904149346\t [0.76911268 2.62085874]\n"
     ]
    }
   ],
   "source": [
    "# for i in range(0,5):\n",
    "#     print(x[0][i],end='')\n",
    "#     print('\\t',x[1][i],end='')\n",
    "#     print('\\t',x[2][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.72607853, 5.46300603, 7.00925752, 5.12866259, 5.44656811])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
